============ Glob Dataset ===========
#!/bin/bash
DATA_DIR="/home/hengshiou/Documents/chameleon_recsys/Glob" && \
python3 -m acr.preprocessing.acr_preprocess_gcom \
	--input_articles_csv_path ${DATA_DIR}/document_g1/articles_metadata.csv \
 	--input_word_embeddings_path ${DATA_DIR}/Portuguese_word2vec_skip_gram_300/skip_s300.txt \
 	--vocab_most_freq_words 50000 \
 	--output_word_vocab_embeddings_path ${DATA_DIR}/pickles/acr_word_vocab_embeddings.pickle \
 	--output_label_encoders ${DATA_DIR}/pickles/acr_label_encoders.pickle \
 	--output_tf_records_path "${DATA_DIR}/articles_tfrecords/gcom_articles_tokenized_*.tfrecord.gz" \
 	--articles_by_tfrecord 5000


============ Adressa Dataset ===========
DATA_DIR="/home/hengshiou/Documents/chameleon_recsys/adressa" && \
python3 -m acr.preprocessing.acr_preprocess_adressa \
	--input_articles_folder_path ${DATA_DIR}/data/contentdata \
 	--input_word_embeddings_path ${DATA_DIR}/word_embeddings/w2v_skipgram_no_lemma_aviskorpus_nowac_nbdigital/model.txt \
 	--vocab_most_freq_words 100000 \
	--max_words_length 1000 \
 	--output_word_vocab_embeddings_path ${DATA_DIR}/pickles/acr_word_vocab_embeddings.pickle \
 	--output_label_encoders ${DATA_DIR}/pickles/acr_label_encoders.pickle \
 	--output_tf_records_path "${DATA_DIR}/articles_tfrecords/adressa_articles_*.tfrecord.gz" \
	--output_articles_csv_path "${DATA_DIR}/adressa_articles.csv" \
 	--articles_by_tfrecord 1000

===========GRU4Rec==============
DATA_DIR="/home/hengshiou/Downloads/chameleon_recsys-1.7.3" && \
time python3 -m  nar.benchmarks.gru4rec.run_gru4rec \
--dataset_type "adressa" \
--train_set_path_regex "${DATA_DIR}/data_transformed/sessions_tfrecords_by_hour_pivot_4/adressa_sessions_hour_*.tfrecord.gz" \
--eval_sessions_negative_samples_json_path "${DATA_DIR}/2020_04_24_023932_eval_sessions_negative_samples_b4.json" \
--acr_module_resources_path ${DATA_DIR}/data_transformed/pickles/acr_articles_metadata_embeddings.pickle \
--training_hours_for_each_eval 5 \
--eval_metrics_top_n 10 \
--batch_size 128 \
--n_epochs 3 \
--optimizer "adam" \
--dropout_p_hidden 0.0 \
--learning_rate 1e-4 \
--l2_lambda 1e-5 \
--momentum 0.0 \
--embedding 0 \
--recent_clicks_buffer_hours 1.0 \
--recent_clicks_buffer_max_size 30000 \
--recent_clicks_for_normalization 5000 \
--eval_negative_sample_relevance 0.02

============SR-GNN================
DATA_DIR="/home/hengshiou/Downloads/chameleon_recsys-1.7.3" && \
time python3 -m  nar.benchmarks.sr-gnn.run_sr_gnn \
--dataset_type "adressa" \
--train_set_path_regex "${DATA_DIR}/data_transformed/sessions_tfrecords_by_hour_pivot_4/adressa_sessions_hour_*.tfrecord.gz" \
--eval_sessions_negative_samples_json_path "${DATA_DIR}/2020_04_24_023932_eval_sessions_negative_samples_b4.json" \
--acr_module_resources_path ${DATA_DIR}/data_transformed/pickles/acr_articles_metadata_embeddings.pickle \
--training_hours_for_each_eval 5 \
--eval_metrics_top_n 10 \
--batch_size 128 \
--n_epochs 10 \
--hidden_size 200 \
--l2_lambda 1e-5 \
--propagation_steps 1 \
--learning_rate 0.001 \
--learning_rate_decay 0.1 \
--learning_rate_decay_steps 3 \
--nonhybrid \
--recent_clicks_buffer_hours 1.0 \
--recent_clicks_buffer_max_size 30000 \
--recent_clicks_for_normalization 5000 \
--eval_negative_sample_relevance 0.02

DATA_DIR="/home/hengshiou/Documents/chameleon_recsys/adressa" && \
JOB_PREFIX=adressa && \
JOB_ID=`whoami`_${JOB_PREFIX}_`date '+%Y_%m_%d_%H%M%S'` && \
MODEL_DIR='/tmp/chameleon/addressa/jobs/'${JOB_ID} && \
echo 'Running training job and outputing to '${MODEL_DIR} && \
python3 -m acr.acr_trainer_adressa \
	--model_dir ${MODEL_DIR} \
	--train_set_path_regex "${DATA_DIR}/articles_tfrecords/adressa_articles_*.tfrecord.gz" \
	--input_word_vocab_embeddings_path ${DATA_DIR}/pickles/acr_word_vocab_embeddings.pickle \
	--input_label_encoders_path ${DATA_DIR}/pickles/acr_label_encoders.pickle \
	--output_acr_metadata_embeddings_path ${DATA_DIR}/pickles/acr_articles_metadata_embeddings.pickle \
	--batch_size 128 \
	--truncate_tokens_length 300 \
	--training_epochs 5 \
	--learning_rate 3e-4 \
	--dropout_keep_prob 1.0 \
	--l2_reg_lambda 1e-5 \
	--text_feature_extractor "CNN" \
	--training_task "metadata_classification" \
	--cnn_filter_sizes "3,4,5" \
	--cnn_num_filters 128 \
	--rnn_units 512 \
	--rnn_layers 1 \
	--rnn_direction "unidirectional" \
	--acr_embeddings_size 250

DATA_DIR="/home/hengshiou/Documents/chameleon_recsys/adressa" && \
python3 -m nar.preprocessing.nar_preprocess_adressa \
	--input_sessions_json_folder_path ${DATA_DIR}/sessions_processed_by_spark \
	--input_acr_metadata_embeddings_path ${DATA_DIR}/pickles/acr_articles_metadata_embeddings.pickle \
	--input_nar_encoders_dict_path ${DATA_DIR}/pickles/nar_encoders_adressa.pickle \
	--number_hours_to_preprocess 383 \
	--number_pivot_to_preprocess 2 \
 	--output_nar_preprocessing_resources_path ${DATA_DIR}/pickles/nar_preprocessing_resources.pickle \
 	--output_sessions_tfrecords_path "${DATA_DIR}/sessions_tfrecords_by_hour/adressa_sessions_hour_*.tfrecord.gz"

GCP_PROJECT_NAME="chameleon"
GCP_PROJECT_ID="logical-seat-256621"
GCP_REGION=us-central1
GCP_ZONE=us-central1-b
GCS_BUCKET_DATAPROC="dataproc-05fa4ecd-0275-481c-b89e-9cb8506a3de9-us-central1"
DATAPROC_CLUSTER_NAME="tutorial-cluster"
export PROJECT=chameleon;export HOSTNAME="tutorial-cluster-m";export ZONE=us-central1-b

gsutil mb -p ${GCP_PROJECT_NAME} -c regional -l us-central1  gs://${GCS_BUCKET_DATAPROC} 
gcloud dataproc clusters create chameleon-dataproc-cluster \
    --project ${GCP_PROJECT_NAME} \
    --bucket ${GCS_BUCKET_DATAPROC} \
    --image-version 1.3 \
    --region us-central1 \
    --zone ${GCP_ZONE} \
    --num-workers 4 \
    --scopes cloud-platform \
    --initialization-actions gs://dataproc-initialization-actions/jupyter/jupyter.sh \
    --initialization-action-timeout 20m \
    --master-machine-type "n1-highmem-4" \
    --worker-machine-type "n1-standard-4" \
    --worker-machine-type "n1-standard-4" \
    --worker-boot-disk-size=500GB \
    --master-boot-disk-size=500GB

###
python3 -m nar.preprocessing.nar_preprocess_adressa \
	--input_sessions_json_folder_path ${DATA_DIR}/sessions_processed_by_spark \
	--input_acr_metadata_embeddings_path ${DATA_DIR}/pickles/acr_articles_metadata_embeddings.pickle \
	--input_nar_encoders_dict_path ${DATA_DIR}/pickles/nar_encoders_adressa.pickle \
	--number_hours_to_preprocess 383 \
	--number_pivot_to_preprocess 4 \
 	--output_nar_preprocessing_resources_path ${DATA_DIR}/pickles/nar_preprocessing_resources_pivot_4.pickle \
 	--output_sessions_tfrecords_path "${DATA_DIR}/sessions_tfrecords_by_hour_pivot_4/adressa_sessions_hour_*.tfrecord.gz"


####
JOB_PREFIX=adressa && \
JOB_ID=`whoami`_${JOB_PREFIX}_`date '+%Y_%m_%d_%H%M%S'` && \
MODEL_DIR='/tmp/chameleon/jobs/'${JOB_ID} && \
echo 'Running training job and outputing to '${MODEL_DIR} && \
python3 -m nar.nar_trainer_adressa \
	--model_dir ${MODEL_DIR} \
	--train_set_path_regex "${DATA_DIR}/data_transformed/sessions_tfrecords_by_hour/adressa_sessions_*.tfrecord.gz" \
	--train_files_from 0 \
	--train_files_up_to 1 \
	--training_hours_for_each_eval 1 \
	--save_results_each_n_evals 1 \
	--acr_module_resources_path ${DATA_DIR}/data_transformed/pickles/acr_articles_metadata_embeddings.pickle \
	--nar_module_preprocessing_resources_path ${DATA_DIR}/data_transformed/pickles/nar_preprocessing_resources.pickle \
	--batch_size 64 \
	--truncate_session_length 20 \
	--learning_rate 0.0003 \
	--dropout_keep_prob 1.0 \
	--reg_l2 0.0001 \
	--softmax_temperature 0.2 \
	--recent_clicks_buffer_hours 1.0 \
	--recent_clicks_buffer_max_size 20000 \
	--recent_clicks_for_normalization 5000 \
	--eval_metrics_top_n 5 \
	--CAR_embedding_size 1024 \
	--rnn_units 10 \
	--rnn_num_layers 1 \
	--train_total_negative_samples 7 \
	--train_negative_samples_from_buffer 10 \
	--eval_total_negative_samples 7 \
	--eval_negative_samples_from_buffer 10 \
	--eval_negative_sample_relevance 0.1 \
	--enabled_articles_input_features_groups "category,author" \
	--enabled_clicks_input_features_groups "time,device,location,referrer" \
	--enabled_internal_features "recency,novelty,article_content_embeddings,item_clicked_embeddings" \
	--novelty_reg_factor 0.0 \
	--disable_eval_benchmarks

##### 
# The `gcloud ml-engine` commands have been renamed and will soon be removed. Please use `gcloud ai-platform` instead.
#####
PROJECT_ID="logical-seat-256621" && \
DATA_DIR="gs://dataproc-05fa4ecd-0275-481c-b89e-9cb8506a3de9-us-central1/adressa_bucket" && \
JOB_PREFIX=adressa_nar && \
JOB_ID=`whoami`_${JOB_PREFIX}_`date '+%Y_%m_%d_%H%M%S'` && \
MODEL_DIR="gs://chameleon_jobs/gcom/nar_module/${JOB_ID}" && \
JOBS_STAGING_DIR="gs://mlengine_staging/" && \
echo 'Running training job '${JOB_ID} && \
gcloud --project ${PROJECT_ID} ai-platform jobs submit training ${JOB_ID} \
	--package-path nar \
	--module-name nar.nar_trainer_adressa \
	--staging-bucket ${JOBS_STAGING_DIR} \
	--region us-central1 \
	--python-version 3.5 \
	--runtime-version 1.14 \
	--scale-tier basic-gpu \
	--job-dir ${MODEL_DIR} \
	-- \
	--model_dir ${MODEL_DIR} \
	--use_local_cache_model_dir \
	--train_set_path_regex  "${DATA_DIR}/data_transformed/sessions_tfrecords_by_hour/adressa_sessions_*.tfrecord.gz" \
	--train_files_from 0 \
	--train_files_up_to 50 \
	--training_hours_for_each_eval 1 \
	--save_results_each_n_evals 1 \
	--acr_module_resources_path ${DATA_DIR}/data_transformed/pickles/acr_articles_metadata_embeddings.pickle \
	--nar_module_preprocessing_resources_path ${DATA_DIR}/data_transformed/pickles/nar_preprocessing_resources.pickle \
	--truncate_session_length 20 \
	--batch_size 64 \
	--learning_rate 3e-4 \
	--dropout_keep_prob 1.0 \
	--reg_l2 1e-4 \
	--softmax_temperature 0.2 \
	--recent_clicks_buffer_hours 1.0 \
	--recent_clicks_buffer_max_size 30000 \
	--recent_clicks_for_normalization 5000 \
	--eval_metrics_top_n 10 \
	--CAR_embedding_size 1024 \
	--rnn_units 255 \
	--rnn_num_layers 2 \
	--train_total_negative_samples 50 \
	--train_negative_samples_from_buffer 5000 \
	--eval_total_negative_samples 50 \
	--eval_negative_samples_from_buffer 5000 \
	--eval_negative_sample_relevance 0.02 \
	--content_embedding_scale_factor 2.0 \
	--enabled_articles_input_features_groups "category,author" \
	--enabled_clicks_input_features_groups "time,device,location,referrer" \
	--enabled_internal_features "item_clicked_embeddings,recency,novelty,article_content_embeddings" \
	--novelty_reg_factor 0.0 \
    --save_histograms \
	--disable_eval_benchmarks

"""
GCN 版本
"""
"""
Data train local 
"""
DATA_DIR=/home/hengshiou/Documents/chameleon_recsys/adressa && \
JOB_PREFIX=adressa && \
JOB_ID=`whoami`_${JOB_PREFIX}_`date '+%Y_%m_%d_%H%M%S'` && \
MODEL_DIR='/tmp/chameleon/jobs/'${JOB_ID} && \
echo 'Running training job and outputing to '${MODEL_DIR} && \
python3 -m nar.nar_trainer_adressa \
	--model_dir ${MODEL_DIR} \
	--train_set_path_regex "${DATA_DIR}/data_transformed/sessions_tfrecords_by_hour/adressa_sessions_*.tfrecord.gz" \
	--train_files_from 0 \
	--train_files_up_to 383 \
	--training_hours_for_each_eval 5 \
	--save_results_each_n_evals 1 \
	--acr_module_resources_path ${DATA_DIR}/data_transformed/pickles/acr_articles_metadata_embeddings.pickle \
	--kg_module_resources_path_entity ${DATA_DIR}/data_transformed/kge/article_encoded_id_entity_embedding_transe_20.vec \
	--kg_module_resources_path_context ${DATA_DIR}/data_transformed/kge/article_encoded_id_context_embedding_transe_20.vec \
	--word_graph_embedding_resources_path ${DATA_DIR}/data_transformed/pickles/word_graph_x_a_10_100_300.vec \
	--nar_module_preprocessing_resources_path ${DATA_DIR}/data_transformed/pickles/nar_preprocessing_resources.pickle \
	--batch_size 64 \
	--truncate_session_length 20 \
	--learning_rate 3e-4 \
	--dropout_keep_prob 1.0 \
	--reg_l2 1e-4 \
	--softmax_temperature 0.2 \
	--recent_clicks_buffer_hours 1.0 \
	--recent_clicks_buffer_max_size 30000 \
	--recent_clicks_for_normalization 5000 \
	--eval_metrics_top_n 10 \
	--CAR_embedding_size 1024 \
	--rnn_units 255 \
	--rnn_num_layers 2 \
	--train_total_negative_samples 50 \
	--train_negative_samples_from_buffer 5000 \
	--eval_total_negative_samples 50 \
	--eval_negative_samples_from_buffer 5000 \
	--eval_negative_sample_relevance 0.02 \
	--content_embedding_scale_factor 2.0 \
	--enabled_articles_input_features_groups "category,author" \
	--enabled_clicks_input_features_groups "time,device,location,referrer" \
	--enabled_internal_features "recency,novelty,article_content_embeddings,item_clicked_embeddings" \
	--novelty_reg_factor 0.0 \
	--disable_eval_benchmarks \
	--enabled_articles_kg_features "entity,context" \
	--kge_vec_dim 20 \
	--kge_strategy "Append" \
	-- \
	--gcn_layer1_size_article_embedding 250 \
	--gcn_layer2_size_article_embedding 150 \
	--gcn_layer1_size_user_items_contextual 60 \
	--gcn_layer2_size_user_items_contextual 30 \
	--gcn_layer1_size_input_items_features 250 \
	--gcn_layer2_size_input_items_features 125 \
	-- \
	--enabled_attention_LSTM \
	--enabled_gcn_refine_input_items_features \
	--save_eval_sessions_negative_samples \

	//--enabled_articles_kg_features "entity,context" \
	//--kge_strategy "Append,Add,Concat,Concat_transformed" \
	//--enabled_attention_LSTM \
	//--save_eval_sessions_negative_samples \
	//--enabled_gcn_refine_user_items_contextual \
	//--enabled_word_graph_article_content_embedding \

"""
Data train MLEngine
"""
PROJECT_ID="logical-seat-256621" && \
DATA_DIR="gs://dataproc-05fa4ecd-0275-481c-b89e-9cb8506a3de9-us-central1/adressa_bucket" && \
JOB_PREFIX=adressa_nar && \
JOB_ID=`whoami`_${JOB_PREFIX}_`date '+%Y_%m_%d_%H%M%S'` && \
MODEL_DIR="gs://chameleon_jobs/gcom/nar_module/${JOB_ID}" && \
JOBS_STAGING_DIR="gs://mlengine_staging/" && \
echo 'Running training job '${JOB_ID} && \
gcloud --project ${PROJECT_ID} ml-engine jobs submit training ${JOB_ID} \
	--package-path nar \
	--module-name nar.nar_trainer_adressa \
	--staging-bucket ${JOBS_STAGING_DIR} \
	--region us-central1 \
	--python-version 3.5 \
	--runtime-version 1.14 \
	--scale-tier basic-gpu \
	--job-dir ${MODEL_DIR} \
	-- \
	--model_dir ${MODEL_DIR} \
	--use_local_cache_model_dir \
	--train_set_path_regex  "${DATA_DIR}/data_transformed/sessions_tfrecords_by_hour/adressa_sessions_*.tfrecord.gz" \
	--train_files_from 0 \
	--train_files_up_to 59 \
	--training_hours_for_each_eval 1 \
	--save_results_each_n_evals 1 \
	--acr_module_resources_path ${DATA_DIR}/data_transformed/pickles/acr_articles_metadata_embeddings.pickle \
	--kg_module_resources_path_entity ${DATA_DIR}/data_transformed/kge/article_encoded_id_entity_embedding.vec \
	--kg_module_resources_path_context ${DATA_DIR}/data_transformed/kge/article_encoded_id_context_embedding.vec \
	--nar_module_preprocessing_resources_path ${DATA_DIR}/data_transformed/pickles/nar_preprocessing_resources.pickle \
	--truncate_session_length 20 \
	--batch_size 64 \
	--learning_rate 3e-4 \
	--dropout_keep_prob 1.0 \
	--reg_l2 1e-4 \
	--softmax_temperature 0.2 \
	--recent_clicks_buffer_hours 1.0 \
	--recent_clicks_buffer_max_size 30000 \
	--recent_clicks_for_normalization 5000 \
	--eval_metrics_top_n 10 \
	--CAR_embedding_size 1024 \
	--rnn_units 255 \
	--rnn_num_layers 2 \
	--train_total_negative_samples 50 \
	--train_negative_samples_from_buffer 5000 \
	--eval_total_negative_samples 50 \
	--eval_negative_samples_from_buffer 5000 \
	--eval_negative_sample_relevance 0.02 \
	--content_embedding_scale_factor 2.0 \
	--enabled_articles_input_features_groups "category,author" \
	--enabled_clicks_input_features_groups "time,device,location,referrer" \
	--enabled_internal_features "item_clicked_embeddings,recency,novelty,article_content_embeddings" \
	--novelty_reg_factor 0.0 \
	--gcn_layer1_size_article_embedding 250 \
	--gcn_layer2_size_article_embedding 150 \
	--disable_eval_benchmarks \
	-- \
	--enabled_articles_kg_features "NONE"
	--enabled_sliding_hours \
	--enabled_gcn_refine_article_embedding \
	--enabled_articles_kg_features "entity,context" \
	--kge_strategy "Aggregate"

	//kge_strategy{"Add", "Concat", "Aggregate"}
	// --enabled_gcn_refine_article_embedding
	// --enabled_gcn_refine_user_items_contextual 
	// --enabled_attention_LSTM \
	
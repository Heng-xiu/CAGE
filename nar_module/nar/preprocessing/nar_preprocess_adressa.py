import argparse
import pandas as pd
import numpy as np
import json
from collections import defaultdict, Counter
from datetime import datetime
import os
import sys

import tensorflow as tf

from ..tf_records_management import save_rows_to_tf_record_file, make_sequential_feature
from ..utils import serialize, deserialize, hash_str_to_int, extract_local_hour_weekday, gini_index

def create_args_parser():
    parser = argparse.ArgumentParser()

    parser.add_argument(
            '--input_sessions_json_folder_path', default='',
            help='Input path of the folder with sessions in JSON lines file, organized by hour (exported by the Spark script - nar_preprocessing_addressa_01_dataproc.ipynb).')

    parser.add_argument(
            '--input_acr_metadata_embeddings_path', default='',
            help='Input path for a pickle with articles metadata and content embeddings, generated by ACR module.')    

    parser.add_argument(
            '--input_nar_encoders_dict_path', default='',
            help='Input path for a pickle with the dictionary encoders for categorical features (exported by the Spark script - nar_preprocessing_addressa_01_dataproc.ipynb)') 

    parser.add_argument(
            '--number_hours_to_preprocess', type=int, default=-1,
            help='Number of hours to preprocess') 

    parser.add_argument(
            '--number_pivot_to_preprocess', type=int, default=2,
            help='Number of pivot to preprocess') 

    parser.add_argument(
            '--output_nar_preprocessing_resources_path', default='',
            help='Output path for a pickle with label encoders and num scalers of clicks data.')
    
    parser.add_argument(
            '--output_sessions_tfrecords_path', default='',
            help='Output path for TFRecords generated with user sessions')

    return parser

def load_acr_module_resources(acr_module_resources_path):
    (acr_label_encoders, articles_metadata_df, content_article_embeddings) = \
              deserialize(acr_module_resources_path)

    articles_metadata_df.set_index('article_id', inplace=False)
    def get_article_text_length(article_id):
        text_length = articles_metadata_df.loc[article_id]['text_length']
        return text_length

    #tf.logging.info("Read ACR label encoders for: {}".format(acr_label_encoders.keys()))  
    #article_id_label_encoder = acr_label_encoders['article_id']
    return get_article_text_length


def load_nar_module_resources(nar_encoders_dict_path):
    nar_encoders_dict = \
              deserialize(nar_encoders_dict_path)

    print("Read NAR label encoders dict for: {}".format(nar_encoders_dict.keys()))

    return nar_encoders_dict

def load_sessions_json_file(json_path):
    with open(json_path, 'r') as fi:
        for line in fi:
            yield json.loads(line)

def load_sessions_hour(session_hour_path):
    sessions = []
    for session_file in os.listdir(session_hour_path):
        session_file_path = os.path.join(session_hour_path, session_file)
        sessions_hour = load_sessions_json_file(session_file_path)
        for session in sessions_hour:
            sessions.append(session)        
    return sessions

def load_sessions_hours(folder_path):
    #Sorting hours directories (treating cases where number of digits is lower. E.x. "session_hour=3" < "session_hour=20")
    hour_folders = sorted([path for path in os.listdir(folder_path) \
                             if os.path.isdir(os.path.join(folder_path,path))], 
                          key=lambda x: "{:0>5}".format(x.split('=')[1]))
    
    for hour_folder in hour_folders:
        hour_index = int(hour_folder.split('=')[1])
        hour_folder_path = os.path.join(folder_path, hour_folder)
        sessions_hour = load_sessions_hour(hour_folder_path)
        yield (hour_index, sessions_hour)
        

numeric_scalers = {
    '_elapsed_ms_since_last_click': {
                 #Set Maximum of 60 min, just to separate returning users, whose elapsed time since last click will be greater than the max 30-min limit for sessions
                 'valid_max': 60 * 60 * 1000.0, 
                 'avg':    789935.7,
                 'stddev': 1371436.0},
    'active_time_secs': {# 來自dataproc.ipynb 當中的 active_time_stats_df 的資料, 可掌握 mean,stddev,max
                 'valid_max': 900.0,
                 'avg':    65.0,
                 'stddev': 69.37},
    'active_time_secs_by_word': {
                 'valid_max': 10.0,
                 'avg':    1.854,
                 'stddev': 1.474}
}

def standardize_num_feature(feature, values):
    scaler_config = numeric_scalers[feature]
    normalizer = lambda x: (min(int(x), scaler_config['valid_max']) - scaler_config['avg']) / scaler_config['stddev']    
    return  list([normalizer(value) for value in values])

def get_cicled_feature_value(value, max_value):
    value_scaled = (value + 0.000001) / max_value
    value_sin = np.sin(2*np.pi*value_scaled)
    value_cos = np.cos(2*np.pi*value_scaled)
    return value_sin, value_cos


def process_session_clicks_features(sessions_hour, get_article_text_length_fn, args):
    sessions = []

    session_count = 0
    clicked_articles_ids = []
    unique_clicked_articles = set()
    #Normalizing numerical features (standardization) and creating time features
    for session in sessions_hour:

        ## truncated session from minum numver(pivot)
        # sessions_df = sessions_df.loc[(sessions_df['session_size'] >= args.number_pivot_to_preprocess)]
        if(session['session_size'] <= args.number_pivot_to_preprocess):
            continue

        session_count += 1
        for click in session['clicks']:
            
            local_hour, local_weekday = extract_local_hour_weekday(click['timestamp']//1000, 
                                                                   "Europe/Oslo")
            #Normalizing weekday feature
            click['weekday'] = (local_weekday+1-3.5)/7
            
            #Transforming the hour in two "cyclic" features, so that the network 
            #can understand, for example, that there is one hour of difference between both 11pm to 0am and from 0am to 1am
            click['time_hour_sin'], click['time_hour_cos'] = get_cicled_feature_value(local_hour, 24)
           
            #Applying standardization on elapsed time
            click['_elapsed_ms_since_last_click'] = standardize_num_feature('_elapsed_ms_since_last_click', [click['_elapsed_ms_since_last_click']])[0]

            #If active_time_secs is not available, use the average
            if 'active_time_secs' not in click:
                click['active_time_secs'] =  numeric_scalers['active_time_secs']['avg']
            #Normalizing reading time by article length (#words)
            click['active_time_secs_by_word'] = click['active_time_secs'] / get_article_text_length_fn(click['article_id'])
            #Applying standardization
            click['active_time_secs_by_word'] = standardize_num_feature('active_time_secs_by_word', [click['active_time_secs_by_word']])[0]
            #Removing unnormalized feature
            del click['active_time_secs']

            #Applying standardization in this feature
            #click['active_time_secs'] = standardize_num_feature('active_time_secs', [click['active_time_secs']])[0]

            #Copying click attributes as lists in the session
            for key in click:
                if key != "user_id":
                    if key not in session:
                        session[key] = [click[key]]
                    else:
                        session[key].append(click[key])

            clicked_articles_ids.append(click['article_id'])
            unique_clicked_articles.add(click['article_id'])

        #Removing clicks property, as its values were copied to individual list columns
        del session['clicks']
        sessions.append(session)

    #Ensuring sessions within the hour are sorted by session id (time)
    sessions_df = pd.DataFrame(sessions).sort_values('session_id')

    #Printing stats
    clicks_by_articles_counter = dict(Counter(clicked_articles_ids))
    clicks_by_articles = np.array(list(clicks_by_articles_counter.values()))
    total_clicks = np.sum(clicks_by_articles)
    clicks_by_articles_norm = clicks_by_articles / total_clicks
    clicks_by_articles_norm_mean = np.mean(clicks_by_articles_norm)
    clicks_by_articles_norm_median = np.median(clicks_by_articles_norm)

    stats = {'session_count': session_count,
             'clicks': total_clicks,
             'clicks_by_session': total_clicks / session_count,
             'unique_articles': len(unique_clicked_articles),
             'clicks_by_article':float(total_clicks)/len(unique_clicked_articles),
             'norm_pop_mean': clicks_by_articles_norm_mean,
             'norm_pop_median': clicks_by_articles_norm_median,
             'gini_index': gini_index(clicks_by_articles.astype(np.float32))
    }

    print("Stats :{}".format(stats))
    
    return sessions_df, stats, clicks_by_articles_counter


def make_sequence_example(row):
    context_features = {        
        'session_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[row['session_id']])),                
        'session_size': tf.train.Feature(int64_list=tf.train.Int64List(value=[row['session_size']])),
        'session_start': tf.train.Feature(int64_list=tf.train.Int64List(value=[row['session_start']])),
        'user_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[row['user_id'].encode()])),        
    }
    
    context = tf.train.Features(feature=context_features)
    
    sequence_features = {
        'event_timestamp': make_sequential_feature(row["timestamp"]),
        #Categorical features
        'item_clicked': make_sequential_feature(row["article_id"]),
        'city': make_sequential_feature(row["city"]),
        'region': make_sequential_feature(row["region"]),
        'country': make_sequential_feature(row["country"]),
        'device': make_sequential_feature(row["device"]),
        'os': make_sequential_feature(row["os"]),
        'referrer_class': make_sequential_feature(row["referrer_class"]),        
        'weekday': make_sequential_feature(row["weekday"], vtype=float),
        'local_hour_sin': make_sequential_feature(row["time_hour_sin"], vtype=float),
        'local_hour_cos': make_sequential_feature(row["time_hour_cos"], vtype=float),        
        'user_elapsed_ms_since_last_click': make_sequential_feature(row["_elapsed_ms_since_last_click"], vtype=float),
        'active_time_secs_by_word': make_sequential_feature(row["active_time_secs_by_word"], vtype=float),
        #To debug
        'url': make_sequential_feature(row["url"], vtype=str),
    }    

    sequence_feature_lists = tf.train.FeatureLists(feature_list=sequence_features)
    
    return tf.train.SequenceExample(feature_lists=sequence_feature_lists,
                                    context=context
                                   )    

def export_sessions_hour_to_tf_records(hour_index, sessions_df, output_path):        
    export_file_template = output_path.replace('*', '{0:04d}')

    print("Exporting hour {} (# sessions: {})".format(hour_index, len(sessions_df)))
    save_rows_to_tf_record_file(map(lambda x: x[1], sessions_df.iterrows()), 
                            make_sequence_example,
                            export_filename=export_file_template.format(hour_index))


def save_nar_preprocessing_resources(output_path, nar_label_encoders_dict, nar_numeric_scalers):
    to_serialize = {'nar_label_encoders': nar_label_encoders_dict, 
                    'nar_numeric_scalers': nar_numeric_scalers}
    serialize(output_path, to_serialize)

def compute_total_clicks_by_article_stats(clicks_by_articles_counters):
    result = defaultdict(int)
    for hour_counters in clicks_by_articles_counters:
        for article_key in hour_counters.keys():
            result[article_key] += hour_counters[article_key]
    return result

def main():
    parser = create_args_parser()
    args = parser.parse_args()

    
    print('Loading resources generated ACR module (articles metadata)')
    get_article_text_length_fn = load_acr_module_resources(args.input_acr_metadata_embeddings_path)
    #get_article_text_length_fn = None

    print('Loading resources generated by the first step of NAR preprocessing (cat. features dict encoders)')
    nar_encoders_dict = load_nar_module_resources(args.input_nar_encoders_dict_path)

    print('Loading sessions from folder: {}'.format(args.input_sessions_json_folder_path))
    print('Exporting TFRecords to: {}'.format(args.output_sessions_tfrecords_path))
    

    clicks_by_articles_counters = []
    for (hour_index, sessions_hour) in load_sessions_hours(args.input_sessions_json_folder_path):        
        print('Processing hour {}'.format(hour_index))

        ####compute_global_metrics(sessions_hour)
        
        
        sessions_hour_df, hour_stats, hour_clicks_by_articles_counter = process_session_clicks_features(sessions_hour, get_article_text_length_fn, args)

        ## temp_path
        tmp_path = '/home/hengshiou/Documents/chameleon_recsys/adressa/'+'sessions_tfrecords_by_hour_pivot_4/'
        sessions_hour_df.to_csv(tmp_path + 'hour-{}-to-debug.csv'.format(hour_index))

        hour_stats['_hour_index'] = hour_index
        # stats.append(hour_stats)#看起來沒有用到 stats 於是先鋒所
        
        clicks_by_articles_counters.append(hour_clicks_by_articles_counter)
        
        export_sessions_hour_to_tf_records(hour_index, sessions_hour_df, 
                                           output_path=args.output_sessions_tfrecords_path)
        print('')

        
        if args.number_hours_to_preprocess >= 0 and hour_index == args.number_hours_to_preprocess:
            break
        
    print()

    
    print('Exporting Categorical Feature encoders and Numeric scalers dicts: {}'.format(args.output_nar_preprocessing_resources_path))    
    save_nar_preprocessing_resources(args.output_nar_preprocessing_resources_path, 
                                     nar_encoders_dict, 
                                     numeric_scalers)
    
if __name__ == '__main__':
    main()
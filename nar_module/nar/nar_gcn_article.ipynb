{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from scipy import sparse\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack, kron\n",
    "import pickle\n",
    "# tf.enable_eager_execution() \n",
    "from multiprocessing import Process,process, Pool, cpu_count, Manager\n",
    "import threading\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old_way to show the logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array([1, 2, 3])\n",
    "arr2 = np.array([4, 5, 6])\n",
    "arr3 = np.array([1,2,3])\n",
    "arr4 = np.array([7,3,1])\n",
    "arr5 = np.array([1,9,6])\n",
    "res_matrix = np.vstack([arr1, arr2, arr3, arr4, arr5])\n",
    "\n",
    "tmp = list(enumerate(res_matrix))\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_dist(vec1,vec2):\n",
    "    \"\"\"\n",
    "    :param vec1:\n",
    "    :param vec2:\n",
    "    :return: the similarity between two vectors\n",
    "    \"\"\"\n",
    "    dist1 = float(np.dot(vec1,vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2)))\n",
    "    return dist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_dist(res_matrix[0],res_matrix[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def coculate_similarity(index, vs_mat, dict_node_neighbor):\n",
    "#     print(\"exec\")\n",
    "#     node_neighbor_list = []\n",
    "#     if cos_dist(vs_mat[index-1], vs_mat[index]) >= 0.5:\n",
    "#         print(\"idx:\", index,\"vs_mat[index-1] =>\", vs_mat[index-1], \"vs_mat[index] =>\", vs_mat[index])\n",
    "#         node_neighbor_list.append(index)\n",
    "#     dict_node_neighbor[node_idx] = node_neighbor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threadPool():\n",
    "    \"\"\"\n",
    "        \n",
    "    \"\"\"\n",
    "    mat = sp.dok_matrix((res_matrix.shape[0],res_matrix.shape[0]), dtype=np.int8) # 73309 means number of articles\n",
    "    dict_node_neighbor = {}\n",
    "    poolsize = 12\n",
    "    pool = Pool(poolsize)\n",
    "    \n",
    "    for idx, val in tmp[1:]: \n",
    "        dict = {}\n",
    "        dict['idx'] = idx\n",
    "        dict['val'] = val\n",
    "        dict['tmp'] = tmp[idx+1:]\n",
    "        dict['dict_node_neighbor'] = dict_node_neighbor\n",
    "        requests = pool.map(coculate_similarity, dict)\n",
    "    \n",
    "#     # start thread\n",
    "#         for idx, val in tmp[1:]:\n",
    "# #         threading.Thread(target=coculate_similarity, args=(idx, val, tmp[idx+1:], dict_node_neighbor)).start()\n",
    "#             threads[i] = threading.Thread(target=coculate_similarity, args=(idx, val, tmp[idx+1:], dict_node_neighbor))\n",
    "#             threads[i].start()\n",
    "#     #wait finish\n",
    "#     for i in range(len(threads)):\n",
    "#         threads[i].join()\n",
    "    pool.wait()\n",
    "    for user_id, movie_ids in dict_node_neighbor.items():\n",
    "        mat[user_id, movie_ids] = 1\n",
    "    \n",
    "    print(\"d:\",dict_node_neighbor )\n",
    "    return mat.transpose().tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存紀念\n",
    "# def threads():\n",
    "#     \"\"\"\n",
    "#        cpu-comsuming \n",
    "#     \"\"\"\n",
    "#     mat = sp.dok_matrix((res_matrix.shape[0],res_matrix.shape[0]), dtype=np.int8) # 73309 means number of articles\n",
    "#     dict_node_neighbor = {}\n",
    "    \n",
    "# #     # start thread\n",
    "#         for idx, val in tmp[1:]:\n",
    "# #         threading.Thread(target=coculate_similarity, args=(idx, val, tmp[idx+1:], dict_node_neighbor)).start()\n",
    "#             threads[i] = threading.Thread(target=coculate_similarity, args=(idx, val, tmp[idx+1:], dict_node_neighbor))\n",
    "#             threads[i].start()\n",
    "# #     #wait finish\n",
    "#     for i in range(len(threads)):\n",
    "#         threads[i].join()\n",
    "#     for user_id, movie_ids in dict_node_neighbor.items():\n",
    "#         mat[user_id, movie_ids] = 1\n",
    "    \n",
    "#     print(\"d:\",dict_node_neighbor )\n",
    "#     return mat.transpose().tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 沒用\n",
    "# def double_loop():\n",
    "#     dict_node_neighbor = Manager().dict()\n",
    "#     mat = sp.dok_matrix((res_matrix.shape[0],res_matrix.shape[0]), dtype=np.int8) # 73309 means number of articles\n",
    "#     pool = Pool(11) # on 8 processors\n",
    "#     try:\n",
    "#         for idx in range(1,len(res_matrix)):\n",
    "#             pool.apply_async(coculate_similarity, (idx, res_matrix[idx+1:], dict_node_neighbor))\n",
    "#         pool.close()\n",
    "#         pool.join()\n",
    "#     finally:\n",
    "#         #convert\n",
    "#         print(dict_node_neighbor)\n",
    "#         for user_id, movie_ids in dict_node_neighbor.items():\n",
    "#             mat[user_id, movie_ids] = 1\n",
    "#         return mat.transpose().tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list\n",
    "tmp_keys = [idx for idx, _ in tmp[:]]\n",
    "for i in tmp_keys[1:]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coculate_similarity(idx, tmp):\n",
    "    node_neighbor_list = []\n",
    "    arch_idx = idx\n",
    "    arch_val = tmp[idx]\n",
    "    \n",
    "    for next_idx, next_val in tmp[idx+1:]:\n",
    "        if cos_dist(arch_val, next_val) >= 0.5:\n",
    "            node_neighbor_list.append(next_idx)\n",
    "    \n",
    "    dict_node_neighbor[arch_idx] = node_neighbor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_loop():\n",
    "    mat = sp.dok_matrix((res_matrix.shape[0],res_matrix.shape[0]), dtype=np.int8) # 73309 means number of articles\n",
    "    dict_node_neighbor = {}\n",
    "    #\n",
    "    for i in tmp_keys[1:]:\n",
    "        coculate_similarity(i, tmp[i:])\n",
    "    #all thread finished\n",
    "    for user_id, movie_ids in dict_node_neighbor.items():\n",
    "        mat[user_id, movie_ids] = 1\n",
    "        \n",
    "    return mat.transpose().tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Protected main function\n",
    "    threadPool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Backup\n",
    "# def threads():\n",
    "#     \"\"\"\n",
    "#         效果顯著，可是我們需要固定size的pool\n",
    "#     \"\"\"\n",
    "#     mat = sp.dok_matrix((res_matrix.shape[0],res_matrix.shape[0]), dtype=np.int8) # 73309 means number of articles\n",
    "#     dict_node_neighbor = {}\n",
    "#     threads = [None] * 20\n",
    "    \n",
    "#     # start thread\n",
    "#     for i in tqdm(range(len(threads))):\n",
    "#         for idx, val in tmp[1:]:\n",
    "# #         threading.Thread(target=coculate_similarity, args=(idx, val, tmp[idx+1:], dict_node_neighbor)).start()\n",
    "#             threads[i] = threading.Thread(target=coculate_similarity, args=(idx, val, tmp[idx+1:], dict_node_neighbor))\n",
    "#             threads[i].start()\n",
    "#     #wait finish\n",
    "#     for i in range(len(threads)):\n",
    "#         threads[i].join()\n",
    "    \n",
    "#     for user_id, movie_ids in dict_node_neighbor.items():\n",
    "#         mat[user_id, movie_ids] = 1\n",
    "    \n",
    "#     print(\"d:\",dict_node_neighbor )\n",
    "#     return mat.transpose().tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## backup\n",
    "# def double_loop():\n",
    "#     mat = sp.dok_matrix((res_matrix.shape[0],res_matrix.shape[0]), dtype=np.int8) # 73309 means number of articles\n",
    "#     for idx, val in tmp[1:]:\n",
    "#         for idx_n, val_n in tmp[idx+1:]:\n",
    "#             if cos_dist(val, val_n) >= 0.5:\n",
    "#                 mat[idx, idx_n] = 1\n",
    "#     return mat.transpose().tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    mat = sp.dok_matrix((73309,73309), dtype=np.int8) # 73309 means number of articles\n",
    "    for article_id, neighbor_ids in tqdm(desired_convert_dict.items() ,desc=\"dict -> sparse matrix\"):\n",
    "        mat[article_id, neighbor_ids] = 1\n",
    "    mat = mat.transpose().tocsr()\n",
    "\n",
    "    # write sparse matrix\n",
    "    # s_m_path = \"/home/hengshiou/Documents/chameleon_recsys/adressa\" + \"/graph/sparse_matrix.npz\"\n",
    "    s_m_path = FLAGS.output_sparse_matrix_resources_path\n",
    "    sp.save_npz(s_m_path, mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized_way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array([1, 2, 3, 5])\n",
    "arr2 = np.array([4, 5, 6, 0])\n",
    "arr3 = np.array([1,2,3,0])\n",
    "arr4 = np.array([0,1,0,0])\n",
    "arr5 = np.array([1,9,6,9])\n",
    "res_matrix = np.vstack([arr1, arr2, arr3, arr4, arr5])\n",
    "res_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_dist(res_matrix[1], res_matrix[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較 row 的方式\n",
    "similarity_rows = cosine_similarity(res_matrix[1].reshape(1, -1), res_matrix[1:], dense_output=False)\n",
    "print(similarity_rows)\n",
    "pass_threshold_rows = np.where(similarity_rows >= 0.5, 1, 0)\n",
    "print(pass_threshold_rows)\n",
    "\n",
    "# 除了第一個不要，後面為 1 的放入到 list 當中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(res_matrix[2].reshape(1, -1), res_matrix[2:], dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(res_matrix[3].reshape(1, -1), res_matrix[3:], dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(res_matrix[1:], dense_output=True)\n",
    "similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2],[3,4]])\n",
    "print(A.shape)\n",
    "np.pad(A, ((0,0),(2,0)), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate res_matrix 先\n",
    "def opt_way():\n",
    "    centroids = []\n",
    "    for idx in range(1, len(res_matrix[:])):\n",
    "        # \n",
    "        similarity_rows = cosine_similarity(res_matrix[idx].reshape(1, -1), res_matrix[idx:], dense_output=False)\n",
    "\n",
    "        # rm self-loop\n",
    "        similarity_rows[0][0] = 0\n",
    "\n",
    "        # add padding\n",
    "        neighbor = np.where(similarity_rows >= 0.5, 1, 0)\n",
    "        zeros = np.zeros(idx-1, dtype=np.int64)\n",
    "        neighbor = np.append(zeros, neighbor)\n",
    "\n",
    "        centroids = vstack((centroids, csr_matrix(neighbor)), format='csr')\n",
    "\n",
    "    sp.save_npz('sparse_matrix_qw.npz', centroids)\n",
    "\n",
    "# centroids.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit opt_way()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = sp.load_npz('sparse_matrix_q.npz')\n",
    "sparse_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrixw = sp.load_npz('sparse_matrix_qw.npz')\n",
    "sparse_matrixw.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除 self-loop\n",
    "np.fill_diagonal(similarity_matrix, 0)\n",
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除小於 threshold, 將留下的轉成 1,0\n",
    "pass_threshold_matrix = np.where(similarity_matrix >= 0.5, 1, 0)\n",
    "pass_threshold_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.csr_matrix(pass_threshold_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#這邊要進行優畫版本\n",
    "def opt_way():\n",
    "    # 取得 similarity_matrix\n",
    "    similarity_matrix = cosine_similarity(res_matrix[1:], dense_output=True)\n",
    "    # 去除 self-loop\n",
    "    np.fill_diagonal(similarity_matrix, 0)\n",
    "    # 去除小於 threshold, 將留下的轉成 1,0\n",
    "    pass_threshold_matrix = np.where(similarity_matrix >= 0.5, 1, 0)\n",
    "    # 轉化成 csr_matrix\n",
    "    mat = sparse.csr_matrix(pass_threshold_matrix)\n",
    "    #write sparse_matrix\n",
    "    sparse.save_npz(\"sparse_matrix.npz\", mat)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_way().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit opt_way()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF.version_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1 = np.array([1, 2, 3, 5])\n",
    "arr2 = np.array([4, 5, 6, 0])\n",
    "arr3 = np.array([1,2,3,0])\n",
    "arr4 = np.array([7,3,1,1])\n",
    "arr5 = np.array([1,9,6,9])\n",
    "res_matrix = np.vstack([arr1, arr2, arr3, arr4, arr5]).astype(np.float32)\n",
    "type(res_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_distances(a, b):\n",
    "    # x shape is n_a * dim\n",
    "    # y shape is n_b * dim\n",
    "    # results shape is n_a * n_b\n",
    "\n",
    "    normalize_a = tf.nn.l2_normalize(a,1)        \n",
    "    normalize_b = tf.nn.l2_normalize(b,1)\n",
    "    distance = 1 - tf.matmul(normalize_a, normalize_b, transpose_b=True)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對照組\n",
    "cosine_similarity(res_matrix[1:], dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = 1 - compute_cosine_distances(res_matrix[1], res_matrix[1:]).numpy()\n",
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將 imput 轉成 tf.tensor 進行測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(5), Dimension(4)])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_matrix\n",
    "data_tf = tf.convert_to_tensor(res_matrix, np.float32)\n",
    "data_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: This function is deprecated. Please call randint(0, 1 + 1) instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1],\n",
       "       [0, 0, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random_integers(0, 1, (3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Select_44:0\", shape=(4, 4), dtype=float32)\n",
      "[[0. 1. 1. 0.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]]\n",
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#全部比較法\n",
    "#@https://stackoverflow.com/questions/57612543/how-to-perform-similarity-function-over-columns-of-a-tensor-in-tensorflow\n",
    "# Normalize the columns of the tensor\n",
    "normalized_tensor = tf.math.l2_normalize(data_tf, axis=0)\n",
    "\n",
    "# Get the dot product between the columns\n",
    "scores = tf.matmul(normalized_tensor, normalized_tensor, transpose_a=True)\n",
    "\n",
    "zero_diag = scores - tf.linalg.diag(tf.linalg.diag_part(scores))\n",
    "triangular = tf.matrix_band_part(zero_diag, 0, -1)\n",
    "# 練習使用 tf.where https://stackoverflow.com/questions/48909198/tensorflow-conditions-check-if-the-values-inside-the-tensor-is-zero-or-greater\n",
    "a = tf.ones_like(triangular)\n",
    "b = tf.zeros_like(triangular)\n",
    "whered_tensor = tf.where(tf.greater(triangular, 0.5), a, b)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     v = sess.run()\n",
    "    print(whered_tensor)\n",
    "    print(whered_tensor.eval())\n",
    "    w1 = tf.size(whered_tensor).eval()\n",
    "    print(w1)\n",
    "    \n",
    "w1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# 放入到 graph\n",
    "#https://stackoverflow.com/questions/26665799/networkx-adding-edges-to-a-graph-from-a-dictionary-with-lists-as-values\n",
    "\n",
    "G = nx.from_dict_of_lists(dict)\n",
    "G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.edges(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[n for n in G.neighbors(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = nx.to_scipy_sparse_matrix(G)\n",
    "oa = nx.adjacency_matrix(G)\n",
    "I = np.matrix(np.eye(A.shape[0]))\n",
    "X = np.matrix([\n",
    "            [i, -i]\n",
    "            for i in range(A.shape[0])\n",
    "        ], dtype=float)\n",
    "feature = X.tolil()\n",
    "A_hat = A + I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape, I, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(oa == A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立 adj_matrix, feature, 最後透過 GCN 跑\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 csr_matrix, 也就是 adj matrix 概念\n",
    "padding_first_sparse_matrixw = sp.load_npz('sparse_matrix_qw.npz')\n",
    "adj = vstack(padding_first_sparse_matrixw[1:], format='csr')\n",
    "adj.todense(), type(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 feature 出來\n",
    "# e.g. features = sp.vstack((allx, tx)).tolil()\n",
    "rows = adj.shape[0]\n",
    "cols = adj.shape[1]\n",
    "feat_x = sparse.csr_matrix(np.random.random((rows, cols)))\n",
    "feat_x.todense(), type(feat_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_scipy_sparse_matrix(adj)\n",
    "nx.draw(G, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義 TF.graph.layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def matmul(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for sparse matrix multiplication.\"\"\"\n",
    "    if sparse:\n",
    "        return tf.sparse_tensor_dense_matmul(x, y)\n",
    "    return tf.matmul(x, y)\n",
    "\n",
    "\n",
    "class GraphConvLayer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim,\n",
    "            output_dim,\n",
    "            activation=None,\n",
    "            use_bias=False,\n",
    "            name=\"graph_conv\"):\n",
    "        \"\"\"Initialise a Graph Convolution layer.\n",
    "        Args:\n",
    "            input_dim (int): The input dimensionality.\n",
    "            output_dim (int): The output dimensionality, i.e. the number of\n",
    "                units.\n",
    "            activation (callable): The activation function to use. Defaults to\n",
    "                no activation function.\n",
    "            use_bias (bool): Whether to use bias or not. Defaults to `False`.\n",
    "            name (str): The name of the layer. Defaults to `graph_conv`.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.name = name\n",
    "\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.w = tf.get_variable(\n",
    "                name='w',\n",
    "                shape=(self.input_dim, self.output_dim),\n",
    "                initializer=tf.initializers.glorot_uniform())\n",
    "\n",
    "            if self.use_bias:\n",
    "                self.b = tf.get_variable(\n",
    "                    name='b',\n",
    "                    initializer=tf.constant(0.1, shape=(self.output_dim,)))\n",
    "\n",
    "    def call(self, adj_norm, x, sparse=False):\n",
    "        x = matmul(x=x, y=self.w, sparse=sparse)  # XW\n",
    "        x = matmul(x=adj_norm, y=x, sparse=True)  # AXW\n",
    "\n",
    "        if self.use_bias:\n",
    "            x = tf.add(x, self.use_bias)          # AXW + B\n",
    "\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)                # activation(AXW + B)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.call(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    # The zeroth element of the tuple contains the cell location of each\n",
    "    # non-zero value in the sparse matrix\n",
    "    # The first element of the tuple contains the value at each cell location\n",
    "    # in the sparse matrix\n",
    "    # The second element of the tuple contains the full shape of the sparse\n",
    "    # matrix\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow placeholders\n",
    "ph = {\n",
    "    'adj_norm': tf.sparse_placeholder(tf.float32, name=\"adj_mat\"),\n",
    "    'x': tf.sparse_placeholder(tf.float32, name=\"x\")\n",
    "}\n",
    "\n",
    "# out_dims_in_each_layer\n",
    "l_sizes = [200, 100, 50]\n",
    "\n",
    "o_fc1 = GraphConvLayer(\n",
    "    input_dim=feat_x.shape[-1],\n",
    "    output_dim=l_sizes[0],\n",
    "    name='fc1',\n",
    "    activation=tf.nn.tanh)(adj_norm=ph['adj_norm'], x=ph['x'], sparse=True)\n",
    "\n",
    "o_fc2 = GraphConvLayer(\n",
    "    input_dim=l_sizes[0],\n",
    "    output_dim=l_sizes[1],\n",
    "    name='fc2',\n",
    "    activation=tf.nn.tanh)(adj_norm=ph['adj_norm'], x=o_fc1)\n",
    "\n",
    "o_fc3 = GraphConvLayer(\n",
    "    input_dim=l_sizes[1],\n",
    "    output_dim=l_sizes[2],\n",
    "    name='fc3',\n",
    "    activation=tf.nn.tanh)(adj_norm=ph['adj_norm'], x=o_fc2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(\n",
    "#     device_count = {'GPU': 0}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "adj_tilde = adj + np.identity(n=adj.shape[0])\n",
    "d_tilde_diag = np.squeeze(np.sum(np.array(adj_tilde), axis=1))\n",
    "d_tilde_inv_sqrt_diag = np.power(d_tilde_diag, -1/2)\n",
    "d_tilde_inv_sqrt = np.diag(d_tilde_inv_sqrt_diag)\n",
    "adj_norm = np.dot(np.dot(d_tilde_inv_sqrt, adj_tilde), d_tilde_inv_sqrt)\n",
    "adj_norm_tuple = sparse_to_tuple(sp.coo_matrix(adj_norm))\n",
    "feat_x_tuple = sparse_to_tuple(sp.coo_matrix((feat_x)))\n",
    "\n",
    "feed_dict = {ph['adj_norm']: adj_norm_tuple,\n",
    "             ph['x']: feat_x_tuple\n",
    "            }\n",
    "\n",
    "outputs = sess.run(o_fc3, feed_dict=feed_dict)\n",
    "# x_min, x_max = outputs[:, 0].min(), outputs[:, 0].max()\n",
    "# y_min, y_max = outputs[:, 1].min(), outputs[:, 1].max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 學習 mupliprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Manager, Pool\n",
    "\n",
    "def f(x, tmp):\n",
    "    return {x: [x*x, x*x*x]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict()\n",
    "pool = Pool(processes=10)\n",
    "results = []\n",
    "# print \"[0, 1, 4,..., 81]\"\n",
    "\n",
    "for i in range(10):\n",
    "    results.append(pool.apply_async(f, args=(i,tmp)))\n",
    "    \n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "for result in results:\n",
    "    d.update(result.get())\n",
    "\n",
    "d.get(9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正式改造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_dist(vec1,vec2):\n",
    "    \"\"\"\n",
    "    :param vec1:\n",
    "    :param vec2:\n",
    "    :return: the similarity between two vectors\n",
    "    \"\"\"\n",
    "    dist1 = float(np.dot(vec1,vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2)))\n",
    "    return dist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coculate_similarity(idx, tmp):\n",
    "    node_neighbor_list = []\n",
    "    arch_tuple = tmp[idx]\n",
    "    arch_idx = arch_tuple[0]\n",
    "    arch_val = arch_tuple[1]\n",
    "    \n",
    "    for next_idx, next_val in tmp[idx+1:]:\n",
    "        if cos_dist(arch_val, next_val) >= 0.5:\n",
    "            node_neighbor_list.append(next_idx-1)\n",
    "    return {arch_idx-1: node_neighbor_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiprocess_threads():\n",
    "    rows = res_matrix[1:].shape[0]\n",
    "    cols = res_matrix[1:].shape[0]\n",
    "    mat = sp.lil_matrix((rows, rows), dtype=np.int8) # 73309 means number of articles\n",
    "    dict_node_neighbor = dict()\n",
    "    pool = Pool()\n",
    "    results = []\n",
    "    tmp_keys = [idx for idx, _ in tmp[:]]\n",
    "    # 嘗試減少數量\n",
    "    tmp_keys = sorted(random.sample(tmp_keys, 3))\n",
    "    #assign task\n",
    "    for i in tqdm(tmp_keys[1:], desc=\"coculate_similarity\"):\n",
    "        results.append(pool.apply_async(coculate_similarity, args=(i, tmp[:])))\n",
    "    \n",
    "    #all thread finished\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    #assign result\n",
    "    for result in tqdm(results,desc=\"assign result -> dict\"):\n",
    "        dict_node_neighbor.update(result.get())\n",
    "    #convert_todok_matrix\n",
    "    for user_id, movie_ids in tqdm(dict_node_neighbor.items(),desc=\"dict -> sparse matrix\"):\n",
    "        mat[user_id, movie_ids] = 1\n",
    "    # remove 1st rows and 1st columns because of padding\n",
    "    return mat.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocess_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lil = multiprocess_threads()\n",
    "lil.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize(filename):\n",
    "    #with open(filename, 'rb') as handle:\n",
    "    # with tf.gfile.Open(filename, 'rb') as handle: # ml warning 不行\n",
    "    with tf.io.gfile.GFile (filename, 'rb') as handle:\n",
    "        # 一直出錯試試看\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_acr_module_resources(acr_module_resources_path):\n",
    "    (acr_label_encoders, articles_metadata_df, content_article_embeddings) = \\\n",
    "              deserialize(acr_module_resources_path)\n",
    "\n",
    "    tf.logging.info(\"Read ACR label encoders for: {}\".format(acr_label_encoders.keys()))\n",
    "    tf.logging.info(\"Read ACR articles metadata: {}\".format(len(articles_metadata_df)))\n",
    "    tf.logging.info(\"Read ACR article content embeddings: {}\".format(content_article_embeddings.shape))\n",
    "\n",
    "    return acr_label_encoders, articles_metadata_df, content_article_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acr_module_resources_path = \"/home/hengshiou/Documents/chameleon_recsys/adressa\" + \"/data_transformed/pickles/acr_articles_metadata_embeddings.pickle\"\n",
    "acr_labels_encoder, articles_metadata_df, content_article_embeddings_matrix = load_acr_module_resources(acr_module_resources_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('B.npz') as X:\n",
    "    mtx, dist, _, _ = [X[i] for i in ('mtx','dist','rvecs','tvecs')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 csr_matrix, 也就是 adj matrix 概念\n",
    "path = \"/home/hengshiou/Documents/chameleon_recsys/adressa\" + \"/graph/sparse_matrix_threadspool.npz\"\n",
    "sparse_matrix = sp.load_npz(path)\n",
    "adj = vstack(sparse_matrix[:], format='csr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 feature 出來\n",
    "# e.g. features = sp.vstack((allx, tx)).tolil()\n",
    "feat_x = content_article_embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For debug\n",
    "acr_labels_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Size_25:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[[1, 1, 1],\n",
    "                [2, 2, 2]],\n",
    "               [[3, 3, 3],\n",
    "                [4, 4, 4]],\n",
    "               [[5, 5, 5],\n",
    "                [6, 6, 6]]])\n",
    "tf.size(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(18,) dtype=int32>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(t, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09752588, -0.41623343,  0.07513655,  0.44961343],\n",
       "       [ 0.80553957,  2.37537563,  0.43870845, -0.41147873],\n",
       "       [ 0.51745362, -1.23249856,  0.38042796, -0.32122999],\n",
       "       [-1.62302301, -0.16005602, -0.91906196, -0.58537578],\n",
       "       [ 1.5297895 ,  1.46007447,  0.34930839, -0.66103211],\n",
       "       [ 0.56105465,  0.89964206,  1.06131516, -0.24052735],\n",
       "       [ 1.50574699,  1.3766082 , -0.40292682, -1.87089957],\n",
       "       [-0.89931482,  0.39722655,  0.35787497, -0.31469828]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(loc=0, scale=1, size=(8, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.matrix([\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 1], \n",
    "    [0, 1, 0, 0],\n",
    "    [1, 0, 1, 0]],\n",
    "    dtype=float\n",
    ")\n",
    "X = np.matrix([\n",
    "            [i, -i]\n",
    "            for i in range(A.shape[0])\n",
    "        ], dtype=float)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
